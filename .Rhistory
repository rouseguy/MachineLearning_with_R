scales_items_df <- merge.data.frame(x = scales_items_df, y = wide_thresholds,
by.x = "scale_id", by.y = "scale_id")
# find the min difficulty for every scale
min_values <- aggregate(x = scales_items_df$difficulty, by = list(scales_items_df$scale_id), FUN = min)
# find the max difficulty for every scale
max_values <- aggregate(x = scales_items_df$difficulty, by = list(scales_items_df$scale_id), FUN = max)
# merge scale_items_df and min_value
merged_df = merge.data.frame(scales_items_df, min_values, by.x = "scale_id", by.y = "Group.1")
# rename column
colnames(merged_df)[colnames(merged_df) == "x"] <- "min_difficulty"
# merge scale_items_df and max_value
merged_df = merge.data.frame(merged_df, max_values, by.x = "scale_id", by.y = "Group.1")
# rename column
colnames(merged_df)[colnames(merged_df) == "x"] <- "max_difficulty"
return (merged_df)
}
#Connect to the database and get the master wide dataset
drv <- dbDriver("PostgreSQL")
con <- dbConnect(drv,dbname="leaderamp",host="localhost",port=5432,user="postgres",password="postgres")
scale_item_df <- bootstrap(con)
View(scale_item_df)
STANDARD_ERROR = dbGetQuery(con, "select value from termination_criteria where name = 'standard_error'")
#Set Std Error to 0.1 for test purpose
STANDARD_ERROR = 0.1
MAX_ITEMS = 10
#Set seed
CAT_SEED = 123
set.seed(123)
#First run
i <- c(1)
item_bank <- scale_item_df[scale_item_df$scale_id == 1 &
scale_item_df$scale_type=='Leader Calibration',]
View(item_bank)
View(scale_item_df)
table(x)
?genPolyMatrix()
x <- genPolyMatrix(nrCat=6,model="PCM")
View(x)
names(x)
?genPattern
data(tcals)
View(tcals)
View(tcals)
genPattern(th=0, scale_item_df, model="PCM")
genPattern(th=0, x, model="PCM")
max(x)
table(genPattern(th=0, x, model="PCM"))
library(catR)
library(tidyr)
library(dplyr)
library(readr)
#Read the datasets
#The estimates of the turk.
#This data frame was generated from table 7.1.1 from the output file
estimate_fortran <- read_csv("~/Documents/Kaggle/LeaderAmp/Working/env_scan_turk.csv")
#the item bank
#This data frame was generated from table 7.5.1 from the output file
item_bank <- read_csv("~/Documents/Kaggle/LeaderAmp/Working/env_scanning_item.csv")
#rasch andrich threshold
#This data frame was generated from table 8.1 from the output file
rasch_andrich_threshold <- read_csv("~/Documents/Kaggle/LeaderAmp/Working/env_scan_andrich_thresholds.csv")
#Items that are to be considered to estimate ability
#Not all items in the item bank are used for computing the estimate.
#From the input file to the FORTRAN code, take all the items
#that aren't commented out (by semi-colon)
items_to_consider <- read_csv("~/Documents/Kaggle/LeaderAmp/Working/items_for_computation.csv")
#Extract the first character of each row.
#If it is semi-colon, ignore it
items_to_consider$first_char <- apply(items_to_consider, 1, function(x) substr(as.character(x),1,1))
items_to_consider$item_num <- c(1:nrow(items_to_consider))
#Create the list of items that are to be considered
items_to_consider_list <- items_to_consider[items_to_consider$first_char!=";","item_num"]
#Turk master file
turk_master <- read_csv("~/Documents/Kaggle/LeaderAmp/Working/turk_master.csv")
#The above file is redundant since
#the estimate_fortran has the num_id for each selected turk
#This file is just to cross-check if these two files match
temp1 <- estimate_fortran %>% left_join(turk_master, by="turk")
#Number of rows match. 293 in estimate_fortran.
#all turks found a match on turk_master.
#now- to check if the IDs match
sum(temp1$num_id - temp1$turk_id)
#sum is 0. All the ids matched
rm(temp1, turk_master)
scale_name="Environmental Scanning"
#One more check. To see if item numbers in estimate_foertran
#matches the items_to_consider_list
unique(!(item_bank$item_num %in% items_to_consider_list))
#Returns true. Both have 158 item lists and they are the same
#Load the responses
turk_responses <- read_csv("~/Documents/Kaggle/LeaderAmp/Working/escan_responses.csv")
#Subset only those turk for whom estimates are to be done
turk_responses <- turk_responses[turk_responses$turk_id %in% estimate_fortran$num_id,  ]
#subset only those questions that are to be considered
items_to_consider_list <- as.list(items_to_consider_list)
turk_responses <- turk_responses[,c(1:4, items_to_consider_list$item_num+4)]
####################################
# End of reading inputs
####################################
####################################
#Data prep for PCM
####################################
item_bank_master_for_pcm <- item_bank[,c(3,1)]
#Sort the dataframe by question number
item_bank_master_for_pcm <- item_bank_master_for_pcm %>%
arrange(item_num)
#For input to PCM, need to add item difficulty and andrich thresholds
rasch_andrich_threshold <- data.frame(t(rasch_andrich_threshold))
rasch_andrich_threshold <- rasch_andrich_threshold[rep(seq_len(nrow(rasch_andrich_threshold)),
each=nrow(item_bank),),]
item_bank_master_for_pcm$delta1 <- item_bank_master_for_pcm$item_difficulty + rasch_andrich_threshold[,1]
item_bank_master_for_pcm$delta2 <- item_bank_master_for_pcm$item_difficulty + rasch_andrich_threshold[,2]
item_bank_master_for_pcm$delta3 <- item_bank_master_for_pcm$item_difficulty + rasch_andrich_threshold[,3]
item_bank_master_for_pcm$delta4 <- item_bank_master_for_pcm$item_difficulty + rasch_andrich_threshold[,4]
item_bank_master_for_pcm$delta5 <- item_bank_master_for_pcm$item_difficulty + rasch_andrich_threshold[,5]
### Create a  data frame to store summary results for each turk_id
# Remember to delete the first row once the code is run
# The first row is a dummy row
leaderSummary <- data.frame(cbind(turk_id=0,
scale_name=scale_name,
estimate=0,
standard_error=0,
lowerCI=0,
upperCI=0))
lapply(leaderSummary, class)
leaderSummary[,-2] <- apply(leaderSummary[,-2], 2,
function(x) as.numeric(as.character(x)))
lapply(leaderSummary, class)
turk_rsp <- turk_responses[3,5:ncol(turk_responses)]
x <- as.matrix(turk_rsp)
table(x)
x <- x - 1
table(x)
y <- as.matrix(item_bank_master_for_pcm[,-c(1,2)])
thetaAfter <- thetaEst(y,
x,
model = "PCM",
D = 1.702,
method = "ML",
range = c(lowerCI, upperCI))
thetaAfter
lowerCI <- min(item_bank$item_difficulty)
upperCI <- max(item_bank$item_difficulty)
thetaAfter <- thetaEst(y,
x,
model = "PCM",
D = 1.702,
method = "ML",
range = c(lowerCI, upperCI))
thetaAfter
semTheta(thetaAfter,
y,
x,
"PCM", 1.702, "ML")
View(turk_responses)
View(estimate_fortran)
library(catR)
library(tidyr)
library(dplyr)
library(readr)
#Read the datasets
#The estimates of the turk.
#This data frame was generated from table 7.1.1 from the output file
estimate_fortran <- read_csv("~/Documents/Kaggle/LeaderAmp/Working/env_scan_turk.csv")
#the item bank
#This data frame was generated from table 7.5.1 from the output file
item_bank <- read_csv("~/Documents/Kaggle/LeaderAmp/Working/env_scanning_item.csv")
#rasch andrich threshold
#This data frame was generated from table 8.1 from the output file
rasch_andrich_threshold <- read_csv("~/Documents/Kaggle/LeaderAmp/Working/env_scan_andrich_thresholds.csv")
#Items that are to be considered to estimate ability
#Not all items in the item bank are used for computing the estimate.
#From the input file to the FORTRAN code, take all the items
#that aren't commented out (by semi-colon)
items_to_consider <- read_csv("~/Documents/Kaggle/LeaderAmp/Working/items_for_computation.csv")
#Extract the first character of each row.
#If it is semi-colon, ignore it
items_to_consider$first_char <- apply(items_to_consider, 1, function(x) substr(as.character(x),1,1))
items_to_consider$item_num <- c(1:nrow(items_to_consider))
#Create the list of items that are to be considered
items_to_consider_list <- items_to_consider[items_to_consider$first_char!=";","item_num"]
#Turk master file
turk_master <- read_csv("~/Documents/Kaggle/LeaderAmp/Working/turk_master.csv")
#The above file is redundant since
#the estimate_fortran has the num_id for each selected turk
#This file is just to cross-check if these two files match
temp1 <- estimate_fortran %>% left_join(turk_master, by="turk")
#Number of rows match. 293 in estimate_fortran.
#all turks found a match on turk_master.
#now- to check if the IDs match
sum(temp1$num_id - temp1$turk_id)
#sum is 0. All the ids matched
rm(temp1, turk_master)
scale_name="Environmental Scanning"
#One more check. To see if item numbers in estimate_foertran
#matches the items_to_consider_list
unique(!(item_bank$item_num %in% items_to_consider_list))
#Returns true. Both have 158 item lists and they are the same
#Load the responses
turk_responses <- read_csv("~/Documents/Kaggle/LeaderAmp/Working/escan_responses.csv")
#Subset only those turk for whom estimates are to be done
turk_responses <- turk_responses[turk_responses$turk_id %in% estimate_fortran$num_id,  ]
#subset only those questions that are to be considered
items_to_consider_list <- as.list(items_to_consider_list)
turk_responses <- turk_responses[,c(1:4, items_to_consider_list$item_num+4)]
####################################
# End of reading inputs
####################################
####################################
#Data prep for PCM
####################################
item_bank_master_for_pcm <- item_bank[,c(3,1)]
#Sort the dataframe by question number
item_bank_master_for_pcm <- item_bank_master_for_pcm %>%
arrange(item_num)
#For input to PCM, need to add item difficulty and andrich thresholds
rasch_andrich_threshold <- data.frame(t(rasch_andrich_threshold))
rasch_andrich_threshold <- rasch_andrich_threshold[rep(seq_len(nrow(rasch_andrich_threshold)),
each=nrow(item_bank),),]
item_bank_master_for_pcm$delta1 <- item_bank_master_for_pcm$item_difficulty + rasch_andrich_threshold[,1]
item_bank_master_for_pcm$delta2 <- item_bank_master_for_pcm$item_difficulty + rasch_andrich_threshold[,2]
item_bank_master_for_pcm$delta3 <- item_bank_master_for_pcm$item_difficulty + rasch_andrich_threshold[,3]
item_bank_master_for_pcm$delta4 <- item_bank_master_for_pcm$item_difficulty + rasch_andrich_threshold[,4]
item_bank_master_for_pcm$delta5 <- item_bank_master_for_pcm$item_difficulty + rasch_andrich_threshold[,5]
### Create a  data frame to store summary results for each turk_id
# Remember to delete the first row once the code is run
# The first row is a dummy row
leaderSummary <- data.frame(cbind(turk_id=0,
scale_name=scale_name,
estimate=0,
standard_error=0,
lowerCI=0,
upperCI=0))
lapply(leaderSummary, class)
leaderSummary[,-2] <- apply(leaderSummary[,-2], 2,
function(x) as.numeric(as.character(x)))
lapply(leaderSummary, class)
####################################
#Estimation of Ability for Turk
####################################
### Need to do loop once the initial test code is ready
for(turk_counter in 1:nrow(turk_responses)){
turk_rsp <- turk_responses[turk_counter,5:ncol(turk_responses)]
#Estimate for first item is computed. Then, a while loop for remaining items
#This is to create the first row of the computational process for each turk
response_counter <- 1
theta = 0
itemBankQuestionsAsked <- item_bank_master_for_pcm[1:response_counter,-c(1,2)]
itemResponseList <- as.vector(turk_rsp[1:response_counter])
lowerCI <- min(item_bank$item_difficulty)
upperCI <- max(item_bank$item_difficulty)
#Compute the updated theta now. Question was asked and
# the response was obtained
x <- as.matrix(turk_rsp[1:response_counter])
x <- x - 1
y <- as.matrix(item_bank_master_for_pcm[1:response_counter,-c(1,2)])
thetaAfter <- thetaEst(y,
x,
model = "PCM",
D = 1.702,
method = "ML",
range = c(lowerCI, upperCI))
#print(thetaAfter)
#Compute standard error the estimate
std_error <- semTheta(thetaAfter,
y,
x,
"PCM", 1.702, "ML")
#print(std_error)
#Compute the updated confidence intervals
lowerCIUpdated <- thetaAfter - 1.702*std_error
upperCIUpdated <- thetaAfter + 1.702*std_error
print(lowerCIUpdated)
print(upperCIUpdated)
#Write the intermediate computation results to a dataframe
leaderComputationResults <- data.frame(cbind(turk_responses[turk_counter, 1],
scale_name =scale_name,
item_num = item_bank_master_for_pcm[response_counter,"item_num"],
item_difficulty = item_bank_master_for_pcm[response_counter,"item_difficulty"],
item_response = as.numeric(itemResponseList[response_counter]),
theta_before = theta,
theta_after = thetaAfter,
lower_ci = lowerCIUpdated,
upper_ci = upperCIUpdated,
standard_error = std_error
))
response_counter <- response_counter + 1
while(response_counter <= length(turk_rsp)){
print(response_counter)
theta <- thetaAfter
itemBankQuestionsAsked <- item_bank_master_for_pcm[1:response_counter,-c(1,2)]
itemResponseList <- as.vector(turk_rsp[1:response_counter])
#Compute the updated theta now. Question was asked and
# the response was obtained
x <- as.matrix(turk_rsp[1:response_counter])
x <- x - 1
y <- as.matrix(item_bank_master_for_pcm[1:response_counter,-c(1,2)])
thetaAfter <- thetaEst(y,
x,
model = "PCM",
D = 1.702,
method = "ML",
range = c(lowerCI, upperCI))
#print(thetaAfter)
#Compute standard error the estimate
std_error <- semTheta(thetaAfter,
y,
x,
"PCM", 1.702, "ML")
#print(std_error)
#Compute the updated confidence intervals
lowerCIUpdated <- thetaAfter - 1.702*std_error
upperCIUpdated <- thetaAfter + 1.702*std_error
#print(lowerCIUpdated)
#print(upperCIUpdated)
#Write the intermediate computation results to a dataframe
leaderComputationResults <- data.frame(rbind(leaderComputationResults,
data.frame(cbind(turk_responses[turk_counter, 1],
scale_name =scale_name,
item_num = item_bank_master_for_pcm[response_counter,"item_num"],
item_difficulty = item_bank_master_for_pcm[response_counter,"item_difficulty"],
item_response = as.numeric(itemResponseList[response_counter]),
theta_before = theta,
theta_after = thetaAfter,
lower_ci = lowerCIUpdated,
upper_ci = upperCIUpdated,
standard_error = std_error
))))
response_counter <- response_counter + 1
}
leaderSummary <- data.frame(rbind(leaderSummary,
data.frame(cbind(turk_id=turk_responses[turk_counter, 1],
scale_name=scale_name,
estimate=thetaAfter,
standard_error=std_error,
lowerCI=lowerCIUpdated,
upperCI=upperCIUpdated))))
print(paste("done",turk_counter))
}
View(leaderSummary)
leaderSummary <- leaderSummary[-1, ]
View(leaderSummary)
View(estimate_fortran)
View(item_bank)
##########################################
View(item_bank)
View(estimate_fortran)
names(leaderSummary)
names(estimate_fortran)
ability_correlation <- merge(leaderSummary,
estimate_fortran[,-4],
by.x="turk_id",
by.y="num_id")
View(ability_correlation)
names(estimate_fortran)
names(estimate_fortran)[1:2] <- c("fortran_estimate", "fortran_std_err")
ability_correlation <- merge(leaderSummary,
estimate_fortran[,-4],
by.x="turk_id",
by.y="num_id")
View(ability_correlation)
?cor
cor(ability_correlation$estimate, ability_correlation$fortran_estimate)
?plot
plot(ability_correlation$estimate, ability_correlation$fortran_estimate)
write.csv(ability_correlation, "~/Documents/Kaggle/LeaderAmp/Working/catR_testResults_18Feb2016.csv", row.names=F)
data(mtcars)
library(dplyr)
sample_n(mtcars, 10)
?sample_n
train <- read.csv("~/Documents/ML_with_R/data/bank_marketing.csv")
View(train)
names(train)
train <- read.csv("~/Documents/ML_with_R/data/bank_marketing.csv")
45211*0.8
train_1 <- train %>% group_by(deposit) %>%
sample_n(35211)
train <- read.csv("~/Documents/ML_with_R/data/bank_marketing.csv")
train_1 <- train %>% group_by(deposit) %>%
sample_n(35211)
names(train)
train <- read.csv("~/Documents/ML_with_R/data/bank_marketing.csv")
train_1 <- train %>% group_by(deposit) %>%
sample_n(35211)
table(train$deposit)
train_1 <- train %>% group_by(deposit) %>%
sample_n(2000)
View(train_1)
table(train_1$deposit)
train_1 <- train %>% group_by(deposit) %>%
sample_frac(0.1)
table(train_1$deposit)
train_1 <- train %>% group_by(deposit) %>%
sample_frac(0.2)
table(train_1$deposit)
train_1 <- train %>% group_by(deposit) %>%
sample_frac(0.25)
table(train_1$deposit)
train_1 <- train %>% group_by(deposit) %>%
sample_frac(0.225)
table(train_1$deposit)
train_1 <- train %>% group_by(deposit) %>%
sample_frac(0.223)
table(train_1$deposit)
train_1 <- train %>% group_by(deposit) %>%
sample_frac(0.2221)
table(train_1$deposit)
train_1 <- train %>% group_by(deposit) %>%
sample_frac(0.221)
table(train_1$deposit)
train_1 <- train %>% group_by(deposit) %>%
sample_frac(0.2215)
train_1 <- train %>% group_by(deposit) %>%
sample_frac(0.2213)
train_1 <- train %>% group_by(deposit) %>%
sample_frac(0.2212)
train_1 <- train %>% group_by(deposit) %>%
sample_frac(0.22115)
train_1 <- train %>% group_by(deposit) %>%
sample_frac(0.22117)
library(dplyr)
input <- read.csv("~/Documents/ML_with_R/data/bank_marketing.csv")
input$index <- c(1:nrow(input))
View(input)
test <- train %>% group_by(deposit) %>%
sample_frac(0.22117)
test <- input %>% group_by(deposit) %>%
sample_frac(0.22117)
train <- input[!(input$index %in% test$index), ]
View(train)
View(test)
table(train$deposit)
table(test$deposit)
31092/4119
8830/1170
train <- train[, -18]
test <- test[, -18]
names(train)
names(test)
write.csv(train, "~/Documents/ML_with_R/data/train.csv", row.names=F)
write.csv(test, "~/Documents/ML_with_R/data/test.csv", row.names=F)
setwd("~/Documents/ML_with_R/MachineLearning_with_R/")
library(readr)
?read_csv
train <- read_csv("train.csv")
setwd("~/Documents/ML_with_R/MachineLearning_with_R/")
train <- read_csv("train.csv")
train <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
View(train)
nrow(train)
ncol(train)
lapply(train, class)
View(test)
library(caret)
?dummyVars()
View(train)
tr1 <- dummyVars(deposit ~ . data=train)
tr1 <- dummyVars(deposit ~ . , data=train)
tr2 <- predict(tr1, newdata=train)
View(tr2)
tr3 <- predict(tr1, newdata=test)
ncol(tr2)
ncol(tr3)
View(tr2)
train <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
colnames(train)[colSums(is.na(train))>0]
colnames(test)[colSums(is.na(test))>0]
apply(is.na(train), 2, any)
is.na(train)
is.na(train)>0
ColSums(is.na(train))>0
train[ColSums(is.na(train))>0]
colSums(is.na(train))>0
colnames(train)[colSums(is.na(train))>0]
oneHotTest <- data.frame(cbind(c("A","B","B","C"), c("B","B","B","A")))
View(oneHotTest)
oneHotTest <- data.frame(cbind(1:4, c("A","B","B","C"), c("B","B","B","A")))
View(oneHotTest)
ontHotEncodedTest <- dummyVars(X1 ~., data=oneHotTest)
ontHotEncodedTest <- predict(dummyVars(X1 ~., data=oneHotTest), oneHotTest)
View(ontHotEncodedTest)
tr1 <- dummyVars(deposit ~ . , data=train)
tr2 <- predict(tr1, newdata=train)
tr3 <- predict(tr1, newdata=test)
View(tr3)
names(tr3)
colnames(tr3)
View(tr3)
summary(train)
table(train$job)
table(train$job)/nrow(train)*100
View(train)
plot(train$age)
?cut
cut(train$age, breaks=c(10,20,30,40,50,60,70,80,90,100))
table(cut(train$age, breaks=c(10,20,30,40,50,60,70,80,90,100)))
plot(train$age)
View(train)
plot(train$balance)
?plot
plot(train$balance, train$deposit)
plot(train$balance, train$age)
plot(train$age, train$balance)
ontHotEncodedTest <- predict(dummyVars(X1 ~., data=oneHotTest), oneHotTest)
View(test)
View(ontHotEncodedTest)
oneHotEncodeModel <- dummyVars(deposit ~ . , data=train)
trainUpdated <- predict(oneHotEncodeModel, newdata=train)
oneHotEncodeModel
testUpdated <- predict(oneHotEncodeModel, newdata=test)
